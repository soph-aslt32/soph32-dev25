# 1

## 強化学習にスポットライトがあたる条件

- 本番環境とシミュレーション環境を一致させることができる
- 

## ChatGPTとの相談

### AIによる新規装置立ち上げ時のロジック生成支援

[1]Chang, Jingru, et al. "Deep reinforcement learning for dynamic flexible job shop scheduling with random job arrival." Processes 10.4 (2022): 760
こちらの論文で，ジョブショップスケジューリング問題を解く際に，DispatchRuleを行動の選択肢としながら，計画を進めるニューラルネットワーク持つエージェントが提案されている．

上記の論文を参考に，例えば既存ロジックを形作るいくつかのロジック群（サブロジック）をどう組み合わせればそれなりに良い計画が立てれるロジックを生成できるかを，AIに提案させる．

入力が新規装置構成/利用方法/...etcであり，出力が使用するサブロジックのリストであるAI.
ただし，これは強化学習ではなくベイズ最適化等の，学習済みモデルが残らない，都度最適化を実行するタイプのAIが適していると思われるので，アイデアとしては微妙か．

### AIによる既存ロジックの置き換え

現状のAIでは，既存ロジックと同等かそれ以下の性能しか達成できておらず，この方針は良くない．

### AIによる例外/異常処理判断の支援

既存ロジックは既存ロジックで今まで通りの開発を行う．
ただし，完全に今まで通りの開発ではなく，一定正常系に対しての対応が完了した段階で，ランニングテスト時に，既存ロジック+強化学習でテスト＋学習を行う．
ランニングテスト環境が一定のランダム性（実運用時に起こるであろう状況を網羅）を保っている状態で，既存ロジックでの計画が一定基準で良くないケースを中心に学習をする．
既存ロジックでの計画がうまくいっていないケースに対して，AIがそれより良い計画を出せるようにすることが目的．

### 人手設計ロジックとAIロジックの融合アプローチ

既存ロジックの出力を初期状態・制約にしてAIが局所的に調整する．
調整後の計画を採用するか，既存ロジックの出力をそのまま採用するかは，固定ロジックで判断．（makespanが短い方を採用等）

既存ロジックの出力計画に対して，どのように計画を調整するかを，

- [1]を参考に，入力が既存計画，行動の選択肢が計画調整ルール，報酬が調整後の計画が元の計画からどれだけ短くなったか，で学習を行う．
  - 計画調整ルールの設計が壁になりそうか．

### 装置ごとのスケジューリング難易度の定義と分類

AIに適した領域/そうでない領域の整理を行い，技術ロードマップを提案する．

### AI導入の限界とリスクの技術報告書化

尻ぬぐい・スケープゴートを回避するために，AI導入ハードルと課題を資料化する．

## 既存ロジックを詳細に分解した上での考察

既存ロジックは，，，

- 木探索のようなことをしながら計画を行う

改善し得る箇所

- 計画成功する可能性のない枝を探索することを減らしたい
  - 無駄な枝を探索し続けた結果，計画時間切れで計画失敗となる
- 例外的状況に対する計画の品質低下
- サブロジックの追加に次ぐ追加により，サブロジック同士の組み合わせによって，計画失敗が相次ぐことがある
